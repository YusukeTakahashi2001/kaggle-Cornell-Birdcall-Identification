{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will import the test data and make a simple prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "There are several pieces of information that we are given about the competition and the test set. Two of the important pieces are quoted below.\n",
    "\n",
    "The following can be found on the [evaluation page](https://www.kaggle.com/c/birdsong-recognition/overview/evaluation):\n",
    "> Submissions will be evaluated based on their row-wise micro averaged F1 score.\n",
    "> \n",
    "> For each row_id/time window, you need to provide a space separated list of the set of birds that made a call beginning or ending in that time window. If there are no bird calls in a time window, use the code nocall.\n",
    "> \n",
    "> There are three sites in the test set. Sites 1 and 2 are labeled in 5 second increments, while site 3 was labeled per audio file due to the time consuming nature of the labeling process.\n",
    "\n",
    "This explains how we need to structure our submission file. There will be several submissions for each test audio file, split by time windows (5 seconds) - unless they are from site 3.\n",
    "\n",
    "The following can be found on the [data page](https://www.kaggle.com/c/birdsong-recognition/data):\n",
    ">The hidden test set audio consists of approximately 150 recordings in mp3 format, each roughly 10 minutes long. The recordings were taken at three separate remote locations. Sites 1 and 2 were labeled in 5 second increments and need matching predictions, but due to the time consuming nature of the labeling process the site 3 files are only labeled at the file level. Accordingly, site 3 has relatively few rows in the test set and needs lower time resolution predictions.\n",
    ">\n",
    ">Two example soundscapes from another data source are also provided to illustrate how the soundscapes are labeled and the hidden dataset folder structure. The two example audio files are BLKFR-10-CPL_20190611_093000.pt540.mp3 and ORANGE-7-CAP_20190606_093000.pt623.mp3. These soundscapes were kindly provided by Jack Dumbacher of the California Academy of Science's Department of Ornithology and Mammology.\n",
    "\n",
    "This is just further information on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Audio\n",
    "\n",
    "Firstly, we need to be able to read in five second windows of the test audio. We can do this using librosa. If the audio is from site 3 then we need to whole audio clip, and we can do this by setting duration to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_clip(path, start_time, duration=5):\n",
    "    return librosa.load(path, offset=start_time, duration=duration)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Test Data\n",
    "\n",
    "The information on the test audio is given in test.csv. We have outputted that below. The test audio is also contained in the test_audio folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>row_id</th>\n",
       "      <th>seconds</th>\n",
       "      <th>audio_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>site_1</td>\n",
       "      <td>site_1_0a997dff022e3ad9744d4e7bbf923288_5</td>\n",
       "      <td>5</td>\n",
       "      <td>0a997dff022e3ad9744d4e7bbf923288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>site_1</td>\n",
       "      <td>site_1_0a997dff022e3ad9744d4e7bbf923288_10</td>\n",
       "      <td>10</td>\n",
       "      <td>0a997dff022e3ad9744d4e7bbf923288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>site_1</td>\n",
       "      <td>site_1_0a997dff022e3ad9744d4e7bbf923288_15</td>\n",
       "      <td>15</td>\n",
       "      <td>0a997dff022e3ad9744d4e7bbf923288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     site                                      row_id  seconds  \\\n",
       "0  site_1   site_1_0a997dff022e3ad9744d4e7bbf923288_5        5   \n",
       "1  site_1  site_1_0a997dff022e3ad9744d4e7bbf923288_10       10   \n",
       "2  site_1  site_1_0a997dff022e3ad9744d4e7bbf923288_15       15   \n",
       "\n",
       "                           audio_id  \n",
       "0  0a997dff022e3ad9744d4e7bbf923288  \n",
       "1  0a997dff022e3ad9744d4e7bbf923288  \n",
       "2  0a997dff022e3ad9744d4e7bbf923288  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_FOLDER = '../input/birdsong-recognition/test_audio/'\n",
    "test_info = pd.read_csv('../input/birdsong-recognition/test.csv')\n",
    "test_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Birds\n",
    "\n",
    "The possible birds can be found in the training set, with the ebird_code feature. Almost all are six letter codes. The first twenty have been outputted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aldfly', 'ameavo', 'amebit', 'amecro', 'amegfi', 'amekes',\n",
       "       'amepip', 'amered', 'amerob', 'amewig', 'amewoo', 'amtspa',\n",
       "       'annhum', 'astfly', 'baisan', 'baleag', 'balori', 'banswa',\n",
       "       'barswa', 'bawwar'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/birdsong-recognition/train.csv')\n",
    "birds = train['ebird_code'].unique()\n",
    "birds[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "\n",
    "With all of the information we have found so far, it is now possible for us to make predictions. This can be done in different ways depending on your model - for this example we will just be selecting random birds. For each row:\n",
    "\n",
    "1. Extract the information from test.csv  \n",
    "2. Load in the correct clip (using librosa)  \n",
    "3. Make a prediction  \n",
    "4. Store the prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(sound_clip, birds):\n",
    "    return np.random.choice(birds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    preds = []\n",
    "    for index, row in test_info.iterrows():\n",
    "        # Get test row information\n",
    "        site = row['site']\n",
    "        start_time = row['seconds'] - 5\n",
    "        row_id = row['row_id']\n",
    "        audio_id = row['audio_id']\n",
    "\n",
    "        # Get the test sound clip\n",
    "        if site == 'site_1' or site == 'site_2':\n",
    "            sound_clip = load_test_clip(TEST_FOLDER + audio_id + '.mp3', start_time)\n",
    "        else:\n",
    "            sound_clip = load_test_clip(TEST_FOLDER + audio_id + '.mp3', 0, duration=None)\n",
    "\n",
    "        # Make the prediction\n",
    "        pred = make_prediction(sound_clip, birds)\n",
    "\n",
    "        # Store prediction\n",
    "        preds.append([row_id, pred])\n",
    "\n",
    "    preds = pd.DataFrame(preds, columns=['row_id', 'birds'])\n",
    "except:\n",
    "    preds = pd.read_csv('../input/birdsong-recognition/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputting Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
