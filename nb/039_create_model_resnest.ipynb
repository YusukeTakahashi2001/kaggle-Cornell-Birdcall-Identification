{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overvew\n",
    "\n",
    "- nb033の改良\n",
    "- この[ディスカッション](https://www.kaggle.com/c/birdsong-recognition/discussion/177551)で mono_to_color がおかしいとの指摘があった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = '039'\n",
    "DEBUG = False\n",
    "PERIOD = 5\n",
    "# PATH_EVENT = './../data_ignore/event/nb017_event_rms/nb017_event_rms.csv'\n",
    "DATASET = '32khz'\n",
    "DIR_MODEL = './../data_ignore/model'\n",
    "PATH_MODEL = './../data_ignore/model/resnest50/resnest50_fast_1s1x64d-d8fbf808.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_str = \"\"\"\n",
    "globals:\n",
    "  seed: 1213\n",
    "  device: cuda\n",
    "  num_epochs: 70\n",
    "  output_dir: /kaggle/training_output/\n",
    "  use_fold: 0\n",
    "  target_sr: 32000\n",
    "\n",
    "dataset:\n",
    "  name: SpectrogramDataset\n",
    "  params:\n",
    "    img_size: 224\n",
    "    melspectrogram_parameters:\n",
    "      n_mels: 128\n",
    "      fmin: 20\n",
    "      fmax: 16000\n",
    "    \n",
    "split:\n",
    "  name: StratifiedKFold\n",
    "  params:\n",
    "    n_splits: 5\n",
    "    random_state: 42\n",
    "    shuffle: True\n",
    "\n",
    "loader:\n",
    "  train:\n",
    "    batch_size: 20\n",
    "    shuffle: True\n",
    "    num_workers: 0\n",
    "    pin_memory: True\n",
    "    drop_last: True\n",
    "  val:\n",
    "    batch_size: 10\n",
    "    shuffle: False\n",
    "    num_workers: 0\n",
    "    pin_memory: True\n",
    "    drop_last: False\n",
    "\n",
    "model:\n",
    "  name: resnest50_fast_1s1x64d\n",
    "  params:\n",
    "    pretrained: True\n",
    "    n_classes: 264\n",
    "\n",
    "loss:\n",
    "  name: BCEWithLogitsLoss\n",
    "  params: {}\n",
    "\n",
    "optimizer:\n",
    "  name: Adam\n",
    "  params:\n",
    "    lr: 0.001\n",
    "\n",
    "scheduler:\n",
    "  name: CosineAnnealingLR\n",
    "  params:\n",
    "    T_max: 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import everything I need :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import typing as tp\n",
    "import logging\n",
    "import cv2 \n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "from fastprogress import progress_bar\n",
    "from contextlib import contextmanager\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import resnest.torch as resnest_torch\n",
    "from torch.nn import Conv2d, Module, Linear, BatchNorm2d, ReLU\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './../src/util/')\n",
    "from const import BIRD_CODE, INV_BIRD_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n",
    "    # Stack X as [X,X,X]\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    X = X - mean\n",
    "    std = std or X.std()\n",
    "    Xstd = X / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    if (_max - _min) > eps:\n",
    "        # Normalize to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V\n",
    "\n",
    "\n",
    "def mono_to_color_mod(X, eps=1e-6):\n",
    "    MEAN = np.array([0.485, 0.456, 0.406])\n",
    "    STD = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Normalize to [0, 255]\n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    V = normalize(V, mean=MEAN, std=STD)\n",
    "    return V\n",
    "\n",
    "def normalize(image, mean, std):\n",
    "    image = (image / 255.0).astype(np.float32)\n",
    "    image = (image - mean) / std\n",
    "    return np.moveaxis(image, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(data.Dataset):\n",
    "    def __init__(self, file_list, img_size=224, \n",
    "                 waveform_transforms=None, spectrogram_transforms=None, melspectrogram_parameters={}):\n",
    "        \n",
    "        self.file_list = file_list  # list of list: [file_path, ebird_code]\n",
    "        self.img_size = img_size\n",
    "        self.waveform_transforms = waveform_transforms\n",
    "        self.spectrogram_transforms = spectrogram_transforms\n",
    "        self.melspectrogram_parameters = melspectrogram_parameters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        wav_path, ebird_code = self.file_list[idx]\n",
    "\n",
    "        y, sr = sf.read(wav_path)\n",
    "#         sr, y = wavfile.read(wav_path)\n",
    "\n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "        else:\n",
    "            len_y = len(y)\n",
    "            effective_length = sr * PERIOD\n",
    "            if len_y < effective_length:\n",
    "                new_y = np.zeros(effective_length, dtype=y.dtype)\n",
    "                start = np.random.randint(effective_length - len_y)\n",
    "                new_y[start:start + len_y] = y\n",
    "                y = new_y.astype(np.float32)\n",
    "            elif len_y > effective_length:\n",
    "                start = np.random.randint(len_y - effective_length)\n",
    "                y = y[start:start + effective_length].astype(np.float32)\n",
    "            else:\n",
    "                y = y.astype(np.float32)\n",
    "\n",
    "        melspec = librosa.feature.melspectrogram(y, sr=sr, **self.melspectrogram_parameters)\n",
    "        melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "\n",
    "        if self.spectrogram_transforms:\n",
    "            melspec = self.spectrogram_transforms(melspec)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        image = mono_to_color(melspec)\n",
    "        height, width, _ = image.shape\n",
    "        image = cv2.resize(image, (int(width * self.img_size / height), self.img_size))\n",
    "        image = np.moveaxis(image, 2, 0)\n",
    "        image = (image / 255.0).astype(np.float32)\n",
    "\n",
    "#         labels = np.zeros(len(BIRD_CODE), dtype=\"i\")\n",
    "        labels = np.zeros(len(BIRD_CODE), dtype=\"f\")\n",
    "        labels[BIRD_CODE[ebird_code]] = 1\n",
    "\n",
    "        return image, labels\n",
    "    \n",
    "class SpectrogramDatasetMod(data.Dataset):\n",
    "    '''mono_to_color_modを使用'''\n",
    "    def __init__(self, file_list, img_size=224, \n",
    "                 waveform_transforms=None, spectrogram_transforms=None, melspectrogram_parameters={}):\n",
    "        \n",
    "        self.file_list = file_list  # list of list: [file_path, ebird_code]\n",
    "        self.img_size = img_size\n",
    "        self.waveform_transforms = waveform_transforms\n",
    "        self.spectrogram_transforms = spectrogram_transforms\n",
    "        self.melspectrogram_parameters = melspectrogram_parameters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        wav_path, ebird_code = self.file_list[idx]\n",
    "\n",
    "        y, sr = sf.read(wav_path)\n",
    "#         sr, y = wavfile.read(wav_path)\n",
    "\n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "        else:\n",
    "            len_y = len(y)\n",
    "            effective_length = sr * PERIOD\n",
    "            if len_y < effective_length:\n",
    "                new_y = np.zeros(effective_length, dtype=y.dtype)\n",
    "                start = np.random.randint(effective_length - len_y)\n",
    "                new_y[start:start + len_y] = y\n",
    "                y = new_y.astype(np.float32)\n",
    "            elif len_y > effective_length:\n",
    "                start = np.random.randint(len_y - effective_length)\n",
    "                y = y[start:start + effective_length].astype(np.float32)\n",
    "            else:\n",
    "                y = y.astype(np.float32)\n",
    "\n",
    "        melspec = librosa.feature.melspectrogram(y, sr=sr, **self.melspectrogram_parameters)\n",
    "        melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "\n",
    "        if self.spectrogram_transforms:\n",
    "            melspec = self.spectrogram_transforms(melspec)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        image = mono_to_color_mod(melspec)\n",
    "        height, width, _ = image.shape\n",
    "        image = cv2.resize(image, (int(width * self.img_size / height), self.img_size))\n",
    "        image = np.moveaxis(image, 2, 0)\n",
    "        image = (image / 255.0).astype(np.float32)\n",
    "\n",
    "#         labels = np.zeros(len(BIRD_CODE), dtype=\"i\")\n",
    "        labels = np.zeros(len(BIRD_CODE), dtype=\"f\")\n",
    "        labels[BIRD_CODE[ebird_code]] = 1\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "class SpectrogramEventRmsDataset(data.Dataset):\n",
    "    def __init__(self, file_list, img_size=224, \n",
    "                 waveform_transforms=None, spectrogram_transforms=None, melspectrogram_parameters={}):\n",
    "        \n",
    "        self.file_list = file_list  # list of list: [file_path, ebird_code]\n",
    "        self.img_size = img_size\n",
    "        self.waveform_transforms = waveform_transforms\n",
    "        self.spectrogram_transforms = spectrogram_transforms\n",
    "        self.melspectrogram_parameters = melspectrogram_parameters\n",
    "        self.df_rms = pd.read_csv(PATH_FEAT)\n",
    "        self.sr_feat = 4\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        wav_path, ebird_code = self.file_list[idx]\n",
    "\n",
    "        y, sr = sf.read(wav_path)\n",
    "        \n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "        else:\n",
    "            len_y = len(y)\n",
    "            effective_length = sr * PERIOD\n",
    "            if len_y < effective_length:\n",
    "                new_y = np.zeros(effective_length, dtype=y.dtype)\n",
    "                start = np.random.randint(effective_length - len_y)\n",
    "                new_y[start:start + len_y] = y\n",
    "                y = new_y.astype(np.float32)\n",
    "            elif len_y > effective_length:\n",
    "                basename = os.path.basename(wav_path)\n",
    "                rms = self.df_rms.query('filename == @basename').librosa_rms.values\n",
    "                x_feat_sec = np.arange(0, len(rms))/self.sr_feat + 1/self.sr_feat\n",
    "                event_mask = rms > 1.3*np.median(rms)\n",
    "                \n",
    "                silent = ~any(event_mask)\n",
    "                if silent:\n",
    "                    # event を検出できなかったらランダムにクロップ\n",
    "                    start = np.random.randint(len_y - effective_length)\n",
    "                    y = y[start:start + effective_length].astype(np.float32)\n",
    "                else:\n",
    "                    choice = random.choice(x_feat_sec[event_mask])\n",
    "                    ed_sec = x_feat_sec[-1]\n",
    "                    st_range_sec = 2.5001\n",
    "                    ed_range_sec = ed_sec - 2.5001\n",
    "                    range_in = (st_range_sec <= choice) & (choice <= ed_range_sec)\n",
    "                    if range_in:\n",
    "                        idxs = np.arange(len_y)\n",
    "                        x_sec = idxs/sr\n",
    "                        mask = (choice - 2.5) < x_sec\n",
    "                        start = idxs[mask][0]\n",
    "                        y = y[start:start + effective_length].astype(np.float32)\n",
    "                    else:\n",
    "                        # ランダムにクロップ\n",
    "                        start = np.random.randint(len_y - effective_length)\n",
    "                        y = y[start:start + effective_length].astype(np.float32)\n",
    "                    \n",
    "                # ----\n",
    "            else:\n",
    "                y = y.astype(np.float32)\n",
    "\n",
    "        melspec = librosa.feature.melspectrogram(y, sr=sr, **self.melspectrogram_parameters)\n",
    "        melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "\n",
    "        if self.spectrogram_transforms:\n",
    "            melspec = self.spectrogram_transforms(melspec)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        image = mono_to_color(melspec)\n",
    "        height, width, _ = image.shape\n",
    "        image = cv2.resize(image, (int(width * self.img_size / height), self.img_size))\n",
    "        image = np.moveaxis(image, 2, 0)\n",
    "        image = (image / 255.0).astype(np.float32)\n",
    "\n",
    "#         labels = np.zeros(len(BIRD_CODE), dtype=\"i\")\n",
    "        labels = np.zeros(len(BIRD_CODE), dtype=\"f\")\n",
    "        labels[BIRD_CODE[ebird_code]] = 1\n",
    "\n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramEventRmsDatasetV3(data.Dataset):\n",
    "    def __init__(self, file_list, img_size=224, \n",
    "                 waveform_transforms=None, spectrogram_transforms=None, melspectrogram_parameters={}):\n",
    "        \n",
    "        self.file_list = file_list  # list of list: [file_path, ebird_code]\n",
    "        self.img_size = img_size\n",
    "        self.waveform_transforms = waveform_transforms\n",
    "        self.spectrogram_transforms = spectrogram_transforms\n",
    "        self.melspectrogram_parameters = melspectrogram_parameters\n",
    "#         self.df_rms = pd.read_csv(PATH_FEAT)\n",
    "        self.df_event = pd.read_csv(PATH_EVENT)\n",
    "        self.sr_feat = 4\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        wav_path, ebird_code = self.file_list[idx]\n",
    "\n",
    "        y, sr = sf.read(wav_path)\n",
    "        \n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "        else:\n",
    "            len_y = len(y)\n",
    "            effective_length = sr * PERIOD\n",
    "            if len_y < effective_length:\n",
    "                new_y = np.zeros(effective_length, dtype=y.dtype)\n",
    "                start = np.random.randint(effective_length - len_y)\n",
    "                new_y[start:start + len_y] = y\n",
    "                y = new_y.astype(np.float32)\n",
    "            elif len_y > effective_length:\n",
    "                basename = os.path.basename(wav_path)\n",
    "                event_sec_list = self.df_event.query('filename == @basename').event_sec_list.to_list()[0]\n",
    "                event_sec_list = self.string_to_list(event_sec_list)\n",
    "                \n",
    "                # on event\n",
    "                if len(event_sec_list) != 0:\n",
    "                    choice = random.choice(event_sec_list)\n",
    "                    # 前から2.5秒、後ろから2.5秒の範囲におさまってるか(境界問題)\n",
    "                    ed_sec = len_y / sr\n",
    "                    st_range_sec = PERIOD/2 + 0.0001\n",
    "                    ed_range_sec = ed_sec - st_range_sec\n",
    "                    range_in = (st_range_sec <= choice) & (choice <= ed_range_sec)\n",
    "                    if range_in:\n",
    "                        start = int((choice - PERIOD/2) * sr)\n",
    "                        y = y[start:start + effective_length].astype(np.float32)\n",
    "                    else:\n",
    "                        # ランダムにクロップ\n",
    "                        start = np.random.randint(len_y - effective_length)\n",
    "                        y = y[start:start + effective_length].astype(np.float32)\n",
    "                # off event\n",
    "                else:\n",
    "                    # event を検出できなかったらランダムにクロップ\n",
    "                    start = np.random.randint(len_y - effective_length)\n",
    "                    y = y[start:start + effective_length].astype(np.float32)\n",
    "                    \n",
    "                # ----\n",
    "            else:\n",
    "                y = y.astype(np.float32)\n",
    "\n",
    "\n",
    "        melspec = librosa.feature.melspectrogram(y, sr=sr, **self.melspectrogram_parameters)\n",
    "        melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "\n",
    "        if self.spectrogram_transforms:\n",
    "            melspec = self.spectrogram_transforms(melspec)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        image = mono_to_color(melspec)\n",
    "        height, width, _ = image.shape\n",
    "        image = cv2.resize(image, (int(width * self.img_size / height), self.img_size))\n",
    "        image = np.moveaxis(image, 2, 0)\n",
    "        image = (image / 255.0).astype(np.float32)\n",
    "\n",
    "#         labels = np.zeros(len(BIRD_CODE), dtype=\"i\")\n",
    "        labels = np.zeros(len(BIRD_CODE), dtype=\"f\")\n",
    "        labels[BIRD_CODE[ebird_code]] = 1\n",
    "\n",
    "        return image, labels\n",
    "    \n",
    "    def string_to_list(self, list_str):\n",
    "        for str_replace in ['\\n', '[', ']']:\n",
    "            list_str = list_str.replace(str_replace, '')\n",
    "\n",
    "        split = list_str.split(' ')\n",
    "        events_num = []\n",
    "        for text in split:\n",
    "            try:\n",
    "                num = np.float32(text)\n",
    "                events_num.append(num)\n",
    "            except:\n",
    "                pass\n",
    "        return events_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders_for_training(dataset_class, args_dataset, args_loader, train_file_list, valid_file_list):\n",
    "    # # make dataset\n",
    "    train_dataset = dataset_class(train_file_list, **args_dataset)\n",
    "    val_dataset = dataset_class(valid_file_list, **args_dataset)\n",
    "    # # make dataloader\n",
    "    train_loader = data.DataLoader(train_dataset, **args_loader[\"train\"])\n",
    "    valid_loader = data.DataLoader(val_dataset, **args_loader[\"val\"])\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplAtConv2d(Module):\n",
    "    \"\"\"Split-Attention Conv2d\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, channels, kernel_size, stride=(1, 1), padding=(0, 0),\n",
    "                 dilation=(1, 1), groups=1, bias=True,\n",
    "                 radix=2, reduction_factor=4,\n",
    "                 rectify=False, rectify_avg=False, norm_layer=None,\n",
    "                 dropblock_prob=0.0, **kwargs):\n",
    "        super(SplAtConv2d, self).__init__()\n",
    "        padding = _pair(padding)\n",
    "        self.rectify = rectify and (padding[0] > 0 or padding[1] > 0)\n",
    "        self.rectify_avg = rectify_avg\n",
    "        inter_channels = max(in_channels*radix//reduction_factor, 32)\n",
    "        self.radix = radix\n",
    "        self.cardinality = groups\n",
    "        self.channels = channels\n",
    "        self.dropblock_prob = dropblock_prob\n",
    "        if self.rectify:\n",
    "            from rfconv import RFConv2d\n",
    "            self.conv = RFConv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n",
    "                                 groups=groups*radix, bias=bias, average_mode=rectify_avg, **kwargs)\n",
    "        else:\n",
    "            self.conv = Conv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n",
    "                               groups=groups*radix, bias=bias, **kwargs)\n",
    "        self.use_bn = norm_layer is not None\n",
    "        if self.use_bn:\n",
    "            self.bn0 = norm_layer(channels*radix)\n",
    "        self.relu = ReLU(inplace=True)\n",
    "        self.fc1 = Conv2d(channels, inter_channels, 1, groups=self.cardinality)\n",
    "        if self.use_bn:\n",
    "            self.bn1 = norm_layer(inter_channels)\n",
    "        self.fc2 = Conv2d(inter_channels, channels*radix, 1, groups=self.cardinality)\n",
    "        if dropblock_prob > 0.0:\n",
    "            self.dropblock = DropBlock2D(dropblock_prob, 3)\n",
    "        self.rsoftmax = rSoftMax(radix, groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn0(x)\n",
    "        if self.dropblock_prob > 0.0:\n",
    "            x = self.dropblock(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        batch, rchannel = x.shape[:2]\n",
    "        if self.radix > 1:\n",
    "            if torch.__version__ < '1.5':\n",
    "                splited = torch.split(x, int(rchannel//self.radix), dim=1)\n",
    "            else:\n",
    "                splited = torch.split(x, rchannel//self.radix, dim=1)\n",
    "            gap = sum(splited) \n",
    "        else:\n",
    "            gap = x\n",
    "        gap = F.adaptive_avg_pool2d(gap, 1)\n",
    "        gap = self.fc1(gap)\n",
    "\n",
    "        if self.use_bn:\n",
    "            gap = self.bn1(gap)\n",
    "        gap = self.relu(gap)\n",
    "\n",
    "        atten = self.fc2(gap)\n",
    "        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)\n",
    "\n",
    "        if self.radix > 1:\n",
    "            if torch.__version__ < '1.5':\n",
    "                attens = torch.split(atten, int(rchannel//self.radix), dim=1)\n",
    "            else:\n",
    "                attens = torch.split(atten, rchannel//self.radix, dim=1)\n",
    "            out = sum([att*split for (att, split) in zip(attens, splited)])\n",
    "        else:\n",
    "            out = atten * x\n",
    "        return out.contiguous()\n",
    "\n",
    "class rSoftMax(nn.Module):\n",
    "    def __init__(self, radix, cardinality):\n",
    "        super().__init__()\n",
    "        self.radix = radix\n",
    "        self.cardinality = cardinality\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.size(0)\n",
    "        if self.radix > 1:\n",
    "            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            x = x.reshape(batch, -1)\n",
    "        else:\n",
    "            x = torch.sigmoid(x)\n",
    "        return x\n",
    "class DropBlock2D(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return nn.functional.adaptive_avg_pool2d(inputs, 1).view(inputs.size(0), -1)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"ResNet Bottleneck\n",
    "    \"\"\"\n",
    "    # pylint: disable=unused-argument\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None,\n",
    "                 radix=1, cardinality=1, bottleneck_width=64,\n",
    "                 avd=False, avd_first=False, dilation=1, is_first=False,\n",
    "                 rectified_conv=False, rectify_avg=False,\n",
    "                 norm_layer=None, dropblock_prob=0.0, last_gamma=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        group_width = int(planes * (bottleneck_width / 64.)) * cardinality\n",
    "        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n",
    "        self.bn1 = norm_layer(group_width)\n",
    "        self.dropblock_prob = dropblock_prob\n",
    "        self.radix = radix\n",
    "        self.avd = avd and (stride > 1 or is_first)\n",
    "        self.avd_first = avd_first\n",
    "\n",
    "        if self.avd:\n",
    "            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n",
    "            stride = 1\n",
    "\n",
    "        if dropblock_prob > 0.0:\n",
    "            self.dropblock1 = DropBlock2D(dropblock_prob, 3)\n",
    "            if radix == 1:\n",
    "                self.dropblock2 = DropBlock2D(dropblock_prob, 3)\n",
    "            self.dropblock3 = DropBlock2D(dropblock_prob, 3)\n",
    "\n",
    "        if radix >= 1:\n",
    "            self.conv2 = SplAtConv2d(\n",
    "                group_width, group_width, kernel_size=3,\n",
    "                stride=stride, padding=dilation,\n",
    "                dilation=dilation, groups=cardinality, bias=False,\n",
    "                radix=radix, rectify=rectified_conv,\n",
    "                rectify_avg=rectify_avg,\n",
    "                norm_layer=norm_layer,\n",
    "                dropblock_prob=dropblock_prob)\n",
    "        elif rectified_conv:\n",
    "            from rfconv import RFConv2d\n",
    "            self.conv2 = RFConv2d(\n",
    "                group_width, group_width, kernel_size=3, stride=stride,\n",
    "                padding=dilation, dilation=dilation,\n",
    "                groups=cardinality, bias=False,\n",
    "                average_mode=rectify_avg)\n",
    "            self.bn2 = norm_layer(group_width)\n",
    "        else:\n",
    "            self.conv2 = nn.Conv2d(\n",
    "                group_width, group_width, kernel_size=3, stride=stride,\n",
    "                padding=dilation, dilation=dilation,\n",
    "                groups=cardinality, bias=False)\n",
    "            self.bn2 = norm_layer(group_width)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            group_width, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = norm_layer(planes*4)\n",
    "\n",
    "        if last_gamma:\n",
    "            from torch.nn.init import zeros_\n",
    "            zeros_(self.bn3.weight)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.dilation = dilation\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        if self.dropblock_prob > 0.0:\n",
    "            out = self.dropblock1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.avd and self.avd_first:\n",
    "            out = self.avd_layer(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        if self.radix == 0:\n",
    "            out = self.bn2(out)\n",
    "            if self.dropblock_prob > 0.0:\n",
    "                out = self.dropblock2(out)\n",
    "            out = self.relu(out)\n",
    "\n",
    "        if self.avd and not self.avd_first:\n",
    "            out = self.avd_layer(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.dropblock_prob > 0.0:\n",
    "            out = self.dropblock3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNet Variants\n",
    "    Parameters\n",
    "    ----------\n",
    "    block : Block\n",
    "        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n",
    "    layers : list of int\n",
    "        Numbers of layers in each block\n",
    "    classes : int, default 1000\n",
    "        Number of classification classes.\n",
    "    dilated : bool, default False\n",
    "        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n",
    "        typically used in Semantic Segmentation.\n",
    "    norm_layer : object\n",
    "        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n",
    "        for Synchronized Cross-GPU BachNormalization).\n",
    "    Reference:\n",
    "        - He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
    "        - Yu, Fisher, and Vladlen Koltun. \"Multi-scale context aggregation by dilated convolutions.\"\n",
    "    \"\"\"\n",
    "    # pylint: disable=unused-variable\n",
    "    def __init__(self, block, layers, radix=1, groups=1, bottleneck_width=64,\n",
    "                 num_classes=1000, dilated=False, dilation=1,\n",
    "                 deep_stem=False, stem_width=64, avg_down=False,\n",
    "                 rectified_conv=False, rectify_avg=False,\n",
    "                 avd=False, avd_first=False,\n",
    "                 final_drop=0.0, dropblock_prob=0,\n",
    "                 last_gamma=False, norm_layer=nn.BatchNorm2d):\n",
    "        self.cardinality = groups\n",
    "        self.bottleneck_width = bottleneck_width\n",
    "        # ResNet-D params\n",
    "        self.inplanes = stem_width*2 if deep_stem else 64\n",
    "        self.avg_down = avg_down\n",
    "        self.last_gamma = last_gamma\n",
    "        # ResNeSt params\n",
    "        self.radix = radix\n",
    "        self.avd = avd\n",
    "        self.avd_first = avd_first\n",
    "\n",
    "        super(ResNet, self).__init__()\n",
    "        self.rectified_conv = rectified_conv\n",
    "        self.rectify_avg = rectify_avg\n",
    "        if rectified_conv:\n",
    "            from rfconv import RFConv2d\n",
    "            conv_layer = RFConv2d\n",
    "        else:\n",
    "            conv_layer = nn.Conv2d\n",
    "        conv_kwargs = {'average_mode': rectify_avg} if rectified_conv else {}\n",
    "        if deep_stem:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                conv_layer(3, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **conv_kwargs),\n",
    "                norm_layer(stem_width),\n",
    "                nn.ReLU(inplace=True),\n",
    "                conv_layer(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
    "                norm_layer(stem_width),\n",
    "                nn.ReLU(inplace=True),\n",
    "                conv_layer(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = conv_layer(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                   bias=False, **conv_kwargs)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n",
    "        if dilated or dilation == 4:\n",
    "            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n",
    "                                           dilation=2, norm_layer=norm_layer,\n",
    "                                           dropblock_prob=dropblock_prob)\n",
    "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
    "                                           dilation=4, norm_layer=norm_layer,\n",
    "                                           dropblock_prob=dropblock_prob)\n",
    "        elif dilation==2:\n",
    "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                           dilation=1, norm_layer=norm_layer,\n",
    "                                           dropblock_prob=dropblock_prob)\n",
    "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
    "                                           dilation=2, norm_layer=norm_layer,\n",
    "                                           dropblock_prob=dropblock_prob)\n",
    "        else:\n",
    "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                           norm_layer=norm_layer,\n",
    "                                           dropblock_prob=dropblock_prob)\n",
    "            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                           norm_layer=norm_layer,\n",
    "                                           dropblock_prob=dropblock_prob)\n",
    "        self.avgpool = GlobalAvgPool2d()\n",
    "        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, norm_layer):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None,\n",
    "                    dropblock_prob=0.0, is_first=True):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            down_layers = []\n",
    "            if self.avg_down:\n",
    "                if dilation == 1:\n",
    "                    down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride,\n",
    "                                                    ceil_mode=True, count_include_pad=False))\n",
    "                else:\n",
    "                    down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1,\n",
    "                                                    ceil_mode=True, count_include_pad=False))\n",
    "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                                             kernel_size=1, stride=1, bias=False))\n",
    "            else:\n",
    "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                                             kernel_size=1, stride=stride, bias=False))\n",
    "            down_layers.append(norm_layer(planes * block.expansion))\n",
    "            downsample = nn.Sequential(*down_layers)\n",
    "\n",
    "        layers = []\n",
    "        if dilation == 1 or dilation == 2:\n",
    "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
    "                                radix=self.radix, cardinality=self.cardinality,\n",
    "                                bottleneck_width=self.bottleneck_width,\n",
    "                                avd=self.avd, avd_first=self.avd_first,\n",
    "                                dilation=1, is_first=is_first, rectified_conv=self.rectified_conv,\n",
    "                                rectify_avg=self.rectify_avg,\n",
    "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
    "                                last_gamma=self.last_gamma))\n",
    "        elif dilation == 4:\n",
    "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
    "                                radix=self.radix, cardinality=self.cardinality,\n",
    "                                bottleneck_width=self.bottleneck_width,\n",
    "                                avd=self.avd, avd_first=self.avd_first,\n",
    "                                dilation=2, is_first=is_first, rectified_conv=self.rectified_conv,\n",
    "                                rectify_avg=self.rectify_avg,\n",
    "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
    "                                last_gamma=self.last_gamma))\n",
    "        else:\n",
    "            raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n",
    "\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes,\n",
    "                                radix=self.radix, cardinality=self.cardinality,\n",
    "                                bottleneck_width=self.bottleneck_width,\n",
    "                                avd=self.avd, avd_first=self.avd_first,\n",
    "                                dilation=dilation, rectified_conv=self.rectified_conv,\n",
    "                                rectify_avg=self.rectify_avg,\n",
    "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
    "                                last_gamma=self.last_gamma))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if self.drop:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class DropBlock2D(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return nn.functional.adaptive_avg_pool2d(inputs, 1).view(inputs.size(0), -1)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"ResNet Bottleneck\n",
    "    \"\"\"\n",
    "    # pylint: disable=unused-argument\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None,\n",
    "                 radix=1, cardinality=1, bottleneck_width=64,\n",
    "                 avd=False, avd_first=False, dilation=1, is_first=False,\n",
    "                 rectified_conv=False, rectify_avg=False,\n",
    "                 norm_layer=None, dropblock_prob=0.0, last_gamma=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        group_width = int(planes * (bottleneck_width / 64.)) * cardinality\n",
    "        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n",
    "        self.bn1 = norm_layer(group_width)\n",
    "        self.dropblock_prob = dropblock_prob\n",
    "        self.radix = radix\n",
    "        self.avd = avd and (stride > 1 or is_first)\n",
    "        self.avd_first = avd_first\n",
    "\n",
    "        if self.avd:\n",
    "            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n",
    "            stride = 1\n",
    "\n",
    "        if dropblock_prob > 0.0:\n",
    "            self.dropblock1 = DropBlock2D(dropblock_prob, 3)\n",
    "            if radix == 1:\n",
    "                self.dropblock2 = DropBlock2D(dropblock_prob, 3)\n",
    "            self.dropblock3 = DropBlock2D(dropblock_prob, 3)\n",
    "\n",
    "        if radix >= 1:\n",
    "            self.conv2 = SplAtConv2d(\n",
    "                group_width, group_width, kernel_size=3,\n",
    "                stride=stride, padding=dilation,\n",
    "                dilation=dilation, groups=cardinality, bias=False,\n",
    "                radix=radix, rectify=rectified_conv,\n",
    "                rectify_avg=rectify_avg,\n",
    "                norm_layer=norm_layer,\n",
    "                dropblock_prob=dropblock_prob)\n",
    "        elif rectified_conv:\n",
    "            from rfconv import RFConv2d\n",
    "            self.conv2 = RFConv2d(\n",
    "                group_width, group_width, kernel_size=3, stride=stride,\n",
    "                padding=dilation, dilation=dilation,\n",
    "                groups=cardinality, bias=False,\n",
    "                average_mode=rectify_avg)\n",
    "            self.bn2 = norm_layer(group_width)\n",
    "        else:\n",
    "            self.conv2 = nn.Conv2d(\n",
    "                group_width, group_width, kernel_size=3, stride=stride,\n",
    "                padding=dilation, dilation=dilation,\n",
    "                groups=cardinality, bias=False)\n",
    "            self.bn2 = norm_layer(group_width)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            group_width, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = norm_layer(planes*4)\n",
    "\n",
    "        if last_gamma:\n",
    "            from torch.nn.init import zeros_\n",
    "            zeros_(self.bn3.weight)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.dilation = dilation\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        if self.dropblock_prob > 0.0:\n",
    "            out = self.dropblock1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.avd and self.avd_first:\n",
    "            out = self.avd_layer(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        if self.radix == 0:\n",
    "            out = self.bn2(out)\n",
    "            if self.dropblock_prob > 0.0:\n",
    "                out = self.dropblock2(out)\n",
    "            out = self.relu(out)\n",
    "\n",
    "        if self.avd and not self.avd_first:\n",
    "            out = self.avd_layer(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.dropblock_prob > 0.0:\n",
    "            out = self.dropblock3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNet Variants\n",
    "    Parameters\n",
    "    ----------\n",
    "    block : Block\n",
    "        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n",
    "    layers : list of int\n",
    "        Numbers of layers in each block\n",
    "    classes : int, default 1000\n",
    "        Number of classification classes.\n",
    "    dilated : bool, default False\n",
    "        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n",
    "        typically used in Semantic Segmentation.\n",
    "    norm_layer : object\n",
    "        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n",
    "        for Synchronized Cross-GPU BachNormalization).\n",
    "    Reference:\n",
    "        - He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
    "        - Yu, Fisher, and Vladlen Koltun. \"Multi-scale context aggregation by dilated convolutions.\"\n",
    "    \"\"\"\n",
    "    # pylint: disable=unused-variable\n",
    "    def __init__(self, block, layers, radix=1, groups=1, bottleneck_width=64,\n",
    "                 num_classes=1000, dilated=False, dilation=1,\n",
    "                 deep_stem=False, stem_width=64, avg_down=False,\n",
    "                 rectified_conv=False, rectify_avg=False,\n",
    "                 avd=False, avd_first=False,\n",
    "                 final_drop=0.0, dropblock_prob=0,\n",
    "                 last_gamma=False, norm_layer=nn.BatchNorm2d):\n",
    "        self.cardinality = groups\n",
    "        self.bottleneck_width = bottleneck_width\n",
    "        # ResNet-D params\n",
    "        self.inplanes = stem_width*2 if deep_stem else 64\n",
    "        self.avg_down = avg_down\n",
    "        self.last_gamma = last_gamma\n",
    "        # ResNeSt params\n",
    "        self.radix = radix\n",
    "        self.avd = avd\n",
    "        self.avd_first = avd_first\n",
    "\n",
    "        super(ResNet, self).__init__()\n",
    "        self.rectified_conv = rectified_conv\n",
    "        self.rectify_avg = rectify_avg\n",
    "        if rectified_conv:\n",
    "            from rfconv import RFConv2d\n",
    "            conv_layer = RFConv2d\n",
    "        else:\n",
    "            conv_layer = nn.Conv2d\n",
    "        conv_kwargs = {'average_mode': rectify_avg} if rectified_conv else {}\n",
    "        if deep_stem:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                conv_layer(3, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **conv_kwargs),\n",
    "                norm_layer(stem_width),\n",
    "                nn.ReLU(inplace=True),\n",
    "                conv_layer(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
    "                norm_layer(stem_width),\n",
    "                nn.ReLU(inplace=True),\n",
    "                conv_layer(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = conv_layer(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                   bias=False, **conv_kwargs)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n",
    "        if dilated or dilation == 4:\n",
    "            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n",
    "                                           dilation=2, norm_layer=norm_layer,\n",
    "                                           dropblock_prob=dropblock_prob)\n",
    "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
    "                                           dilation=4, norm_layer=norm_layer,\n",
    "                                           dropblock_prob=dropblock_prob)\n",
    "        elif dilation==2:\n",
    "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                           dilation=1, norm_layer=norm_layer,\n",
    "                                           dropblock_prob=dropblock_prob)\n",
    "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
    "                                           dilation=2, norm_layer=norm_layer,\n",
    "                                           dropblock_prob=dropblock_prob)\n",
    "        else:\n",
    "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                           norm_layer=norm_layer,\n",
    "                                           dropblock_prob=dropblock_prob)\n",
    "            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                           norm_layer=norm_layer,\n",
    "                                           dropblock_prob=dropblock_prob)\n",
    "        self.avgpool = GlobalAvgPool2d()\n",
    "        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, norm_layer):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None,\n",
    "                    dropblock_prob=0.0, is_first=True):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            down_layers = []\n",
    "            if self.avg_down:\n",
    "                if dilation == 1:\n",
    "                    down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride,\n",
    "                                                    ceil_mode=True, count_include_pad=False))\n",
    "                else:\n",
    "                    down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1,\n",
    "                                                    ceil_mode=True, count_include_pad=False))\n",
    "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                                             kernel_size=1, stride=1, bias=False))\n",
    "            else:\n",
    "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                                             kernel_size=1, stride=stride, bias=False))\n",
    "            down_layers.append(norm_layer(planes * block.expansion))\n",
    "            downsample = nn.Sequential(*down_layers)\n",
    "\n",
    "        layers = []\n",
    "        if dilation == 1 or dilation == 2:\n",
    "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
    "                                radix=self.radix, cardinality=self.cardinality,\n",
    "                                bottleneck_width=self.bottleneck_width,\n",
    "                                avd=self.avd, avd_first=self.avd_first,\n",
    "                                dilation=1, is_first=is_first, rectified_conv=self.rectified_conv,\n",
    "                                rectify_avg=self.rectify_avg,\n",
    "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
    "                                last_gamma=self.last_gamma))\n",
    "        elif dilation == 4:\n",
    "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
    "                                radix=self.radix, cardinality=self.cardinality,\n",
    "                                bottleneck_width=self.bottleneck_width,\n",
    "                                avd=self.avd, avd_first=self.avd_first,\n",
    "                                dilation=2, is_first=is_first, rectified_conv=self.rectified_conv,\n",
    "                                rectify_avg=self.rectify_avg,\n",
    "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
    "                                last_gamma=self.last_gamma))\n",
    "        else:\n",
    "            raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n",
    "\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes,\n",
    "                                radix=self.radix, cardinality=self.cardinality,\n",
    "                                bottleneck_width=self.bottleneck_width,\n",
    "                                avd=self.avd, avd_first=self.avd_first,\n",
    "                                dilation=dilation, rectified_conv=self.rectified_conv,\n",
    "                                rectify_avg=self.rectify_avg,\n",
    "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
    "                                last_gamma=self.last_gamma))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if self.drop:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args: tp.Dict):\n",
    "    # # get resnest50_fast_1s1x64d\n",
    "    model = ResNet(\n",
    "        Bottleneck, [3, 4, 6, 3],\n",
    "        radix=1, groups=1, bottleneck_width=64,\n",
    "        deep_stem=True, stem_width=32, avg_down=True,\n",
    "        avd=True, avd_first=True)\n",
    "    \n",
    "    state_dict = torch.load(args[\"trained_weights\"])\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    del model.fc\n",
    "    # # use the same head as the baseline notebook.\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(2048, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n",
    "        nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n",
    "        nn.Linear(1024, args[\"num_classes\"]))\n",
    "    \n",
    "#     state_dict = torch.load(args[\"trained_weights\"])\n",
    "#     model.load_state_dict(state_dict)\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# def get_model(args: tp.Dict):\n",
    "#     model =getattr(resnest_torch, args[\"name\"])(pretrained=args[\"params\"][\"pretrained\"])\n",
    "#     del model.fc\n",
    "#     # # use the same head as the baseline notebook.\n",
    "#     model.fc = nn.Sequential(\n",
    "#         nn.Linear(2048, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n",
    "#         nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n",
    "#         nn.Linear(1024, args[\"params\"][\"n_classes\"]))\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[globals]\n",
      "{'seed': 1213, 'device': 'cuda', 'num_epochs': 70, 'output_dir': '/kaggle/training_output/', 'use_fold': 0, 'target_sr': 32000}\n",
      "[dataset]\n",
      "{'name': 'SpectrogramDataset', 'params': {'img_size': 224, 'melspectrogram_parameters': {'n_mels': 128, 'fmin': 20, 'fmax': 16000}}}\n",
      "[split]\n",
      "{'name': 'StratifiedKFold', 'params': {'n_splits': 5, 'random_state': 42, 'shuffle': True}}\n",
      "[loader]\n",
      "{'train': {'batch_size': 20, 'shuffle': True, 'num_workers': 0, 'pin_memory': True, 'drop_last': True}, 'val': {'batch_size': 10, 'shuffle': False, 'num_workers': 0, 'pin_memory': True, 'drop_last': False}}\n",
      "[model]\n",
      "{'name': 'resnest50_fast_1s1x64d', 'params': {'pretrained': True, 'n_classes': 264}}\n",
      "[loss]\n",
      "{'name': 'BCEWithLogitsLoss', 'params': {}}\n",
      "[optimizer]\n",
      "{'name': 'Adam', 'params': {'lr': 0.001}}\n",
      "[scheduler]\n",
      "{'name': 'CosineAnnealingLR', 'params': {'T_max': 10}}\n"
     ]
    }
   ],
   "source": [
    "settings = yaml.safe_load(settings_str)\n",
    "if DEBUG:\n",
    "    settings['split']['params']['n_splits'] = 2\n",
    "    settings['globals']['num_epochs'] = 3\n",
    "\n",
    "for k, v in settings.items():\n",
    "    print(\"[{}]\".format(k))\n",
    "    print(v)\n",
    "set_seed(settings[\"globals\"][\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path.cwd().parent\n",
    "input_ex_root = root / 'data_ignore/external_dataset' / DATASET\n",
    "train_resampled_audio_dirs = [input_ex_root / \"birdsong-resampled-train-audio-{:0>2}\"\n",
    "                              .format(i)  for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/user/Git/kaggle-Cornell-Birdcall-Identification/data_ignore/external_dataset/32khz/birdsong-resampled-train-audio-00'),\n",
       " PosixPath('/home/user/Git/kaggle-Cornell-Birdcall-Identification/data_ignore/external_dataset/32khz/birdsong-resampled-train-audio-01'),\n",
       " PosixPath('/home/user/Git/kaggle-Cornell-Birdcall-Identification/data_ignore/external_dataset/32khz/birdsong-resampled-train-audio-02'),\n",
       " PosixPath('/home/user/Git/kaggle-Cornell-Birdcall-Identification/data_ignore/external_dataset/32khz/birdsong-resampled-train-audio-03'),\n",
       " PosixPath('/home/user/Git/kaggle-Cornell-Birdcall-Identification/data_ignore/external_dataset/32khz/birdsong-resampled-train-audio-04')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_resampled_audio_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_resampled_audio_dirs[0] / \"train_mod.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "train_all を作成  \n",
    "train_all: リサンプル後のファイルパスなどを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21375, 38)\n",
      "(21375, 3)\n",
      "(21375, 39)\n"
     ]
    }
   ],
   "source": [
    "tmp_list = []\n",
    "for audio_d in train_resampled_audio_dirs:\n",
    "    if not audio_d.exists():\n",
    "        continue\n",
    "    for ebird_d in audio_d.iterdir():\n",
    "        if ebird_d.is_file():\n",
    "            continue\n",
    "        for i, wav_f in enumerate(ebird_d.iterdir()):\n",
    "            bool_n_splits = i==settings['split']['params']['n_splits']\n",
    "            if bool_n_splits and DEBUG: break  # if DEBUG=True: 1bird/n_splits file\n",
    "            tmp_list.append([ebird_d.name, wav_f.name, wav_f.as_posix()])\n",
    "            \n",
    "train_wav_path_exist = pd.DataFrame(\n",
    "    tmp_list, columns=[\"ebird_code\", \"resampled_filename\", \"file_path\"])\n",
    "\n",
    "del tmp_list\n",
    "\n",
    "train_all = pd.merge(\n",
    "    train, train_wav_path_exist, on=[\"ebird_code\", \"resampled_filename\"], how=\"inner\")\n",
    "\n",
    "if DEBUG: print('----- debug mode -----')\n",
    "print(train.shape)\n",
    "print(train_wav_path_exist.shape)\n",
    "print(train_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(**settings[\"split\"][\"params\"])\n",
    "\n",
    "train_all[\"fold\"] = -1\n",
    "for fold_id, (train_index, val_index) in enumerate(skf.split(train_all, train_all[\"ebird_code\"])):\n",
    "    train_all.iloc[val_index, -1] = fold_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(264, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>fold</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ebird_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aldfly</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ameavo</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amebit</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amecro</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amegfi</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yebsap</th>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yehbla</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yelwar</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yerwar</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yetvir</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "fold         0   1   2   3   4\n",
       "ebird_code                    \n",
       "aldfly      20  20  20  20  20\n",
       "ameavo       8   8   8   7   7\n",
       "amebit       9   9   8   9   9\n",
       "amecro      20  20  20  20  20\n",
       "amegfi      20  20  20  20  20\n",
       "...         ..  ..  ..  ..  ..\n",
       "yebsap      13  12  12  13  13\n",
       "yehbla      11  12  12  11  11\n",
       "yelwar      18  18  17  18  18\n",
       "yerwar      20  20  20  20  20\n",
       "yetvir      19  19  20  20  20\n",
       "\n",
       "[264 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_proportion = pd.pivot_table(train_all, index=\"ebird_code\", columns=\"fold\", values=\"xc_id\", aggfunc=len)\n",
    "print(fold_proportion.shape)\n",
    "fold_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train: 17100, val: 4275\n"
     ]
    }
   ],
   "source": [
    "use_fold = settings[\"globals\"][\"use_fold\"]\n",
    "idx_train = train_all['fold']!=use_fold\n",
    "idx_valid = train_all['fold']==use_fold\n",
    "train_file_list = train_all[idx_train][['file_path', 'ebird_code']].values.tolist()\n",
    "valid_file_list = train_all[idx_valid][['file_path', 'ebird_code']].values.tolist()\n",
    "\n",
    "print(\"[fold {}] train: {}, val: {}\".format(use_fold, len(train_file_list), len(valid_file_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21375"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_list)+len(valid_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(settings['globals']['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**<font color='orange'> -------------------- settings ------------------ </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # get loader\n",
    "train_loader, valid_loader = get_loaders_for_training(\n",
    "    SpectrogramDatasetMod,\n",
    "    settings[\"dataset\"][\"params\"], settings[\"loader\"], train_file_list, valid_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: ResNet\n"
     ]
    }
   ],
   "source": [
    "# resnet18\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# model.fc = nn.Linear(in_features=512, out_features=len(BIRD_CODE))\n",
    "\n",
    "# resnet34\n",
    "# model = models.resnet34(pretrained=True)\n",
    "# model.fc = nn.Linear(in_features=512, out_features=len(BIRD_CODE))\n",
    "\n",
    "# resnet50 のファインチューニング\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# model.fc = nn.Linear(in_features=2048, out_features=len(BIRD_CODE))\n",
    "\n",
    "# resnest\n",
    "# model = getattr(resnest_torch, settings['model'][\"name\"])(pretrained=settings['model'][\"params\"][\"pretrained\"])\n",
    "# del model.fc\n",
    "# model.fc = nn.Sequential(\n",
    "#     nn.Linear(2048, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n",
    "#     nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n",
    "#     nn.Linear(1024, settings['model'][\"params\"][\"n_classes\"]))\n",
    "# resnest\n",
    "model_config = {\n",
    "    \"base_model_name\": \"resnest50_fast_1s1x64d\",\n",
    "    \"pretrained\": False,\n",
    "    \"num_classes\": 264,\n",
    "    \"trained_weights\": PATH_MODEL,\n",
    "}\n",
    "model = get_model(model_config)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "print(f'model name: {model.__class__.__name__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='orange'> ------------------------------------------------ </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # get optimizer\n",
    "optimizer = getattr(\n",
    "        torch.optim, settings[\"optimizer\"][\"name\"]\n",
    "        )(model.parameters(), **settings[\"optimizer\"][\"params\"])\n",
    "\n",
    "# # # get scheduler\n",
    "scheduler = getattr(\n",
    "    torch.optim.lr_scheduler, settings[\"scheduler\"][\"name\"]\n",
    "    )(optimizer, **settings[\"scheduler\"][\"params\"])\n",
    "\n",
    "# # # get loss\n",
    "loss_func = getattr(nn, settings[\"loss\"][\"name\"])(**settings[\"loss\"][\"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, scheduler, loss_func):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(progress_bar(train_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()*data.size(0)\n",
    "    scheduler.step()\n",
    "    loss = epoch_train_loss / len(train_loader.dataset)\n",
    "    del data\n",
    "    return loss\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "#         self.best_state_dict = {}\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "#         self.best_state_dict = model.state_dict()\n",
    "        self.val_loss_min = val_loss\n",
    "        \n",
    "def get_epoch_loss_score(model, device, valid_loder, loss_func):\n",
    "    model.eval()\n",
    "    epoch_valid_loss = 0\n",
    "    y_pred_list = []\n",
    "    y_true_list = []\n",
    "    for batch_idx, (data, target) in enumerate(progress_bar(valid_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        epoch_valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "#         out_numpy = output.detach().cpu().numpy()\n",
    "        _y_pred = output.detach().cpu().numpy().argmax(axis=1)\n",
    "        y_pred_list.append(_y_pred)\n",
    "        _y_true = target.detach().cpu().numpy().argmax(axis=1)\n",
    "        y_true_list.append(_y_true)\n",
    "    \n",
    "    loss = epoch_valid_loss / len(valid_loader.dataset)\n",
    "    y_pred = np.concatenate(y_pred_list, axis=0)\n",
    "    y_true = np.concatenate(y_true_list, axis=0)\n",
    "    f_score = f1_score(y_true, y_pred, average='macro')\n",
    "    del data\n",
    "    return loss, f_score\n",
    "\n",
    "def evaluate(model, device, valid_loder, loss_func):\n",
    "    model.eval()\n",
    "    epoch_valid_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(progress_bar(valid_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = loss_func(output, target)\n",
    "        epoch_valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    loss = epoch_valid_loss / len(valid_loader.dataset)\n",
    "    del data\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'{DIR_MODEL}/nb{NB}_{model.__class__.__name__}/'\n",
    "file_dir = os.path.dirname(save_dir)\n",
    "if not os.path.exists(file_dir):\n",
    "    os.makedirs(file_dir)\n",
    "save_path = f'{save_dir}model_{model.__class__.__name__}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/70 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch: 1 Sat Aug 29 21:38:13 2020\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='855' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/855 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_valid = []\n",
    "epochs = []\n",
    "\n",
    "early_stopping = EarlyStopping(patience=12, verbose=True, path=save_path)\n",
    "n_epoch = settings['globals']['num_epochs']\n",
    "# n_epoch = 50\n",
    "for epoch in progress_bar(range(1, n_epoch+1)):\n",
    "    print(f'\\n epoch: {epoch} {time.ctime()}')\n",
    "    loss_train = train(model, device, train_loader, optimizer, scheduler, loss_func)\n",
    "    loss_valid, f_score_valid = get_epoch_loss_score(model, device, valid_loader, loss_func)\n",
    "    print(f'loss_train: {loss_train:.6f}, loss_valid: {loss_valid:.6f}, f1(macro): {f_score_valid:.6f}')\n",
    "    \n",
    "    epochs.append(epoch)\n",
    "    losses_train.append(loss_train)\n",
    "    losses_valid.append(loss_valid)\n",
    "    \n",
    "    early_stopping(loss_valid, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "# model.load_state_dict(early_stopping.best_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, losses_train, '-x', label='train')\n",
    "plt.plot(epochs, losses_valid, '-x', label='valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_loader\n",
    "# del valid_loader\n",
    "# del model\n",
    "del optimizer\n",
    "del scheduler\n",
    "# del loss_func\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predection check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    pass\n",
    "else:\n",
    "    load_weights = torch.load(save_path)\n",
    "    model.load_state_dict(load_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_valid, f_score_valid = get_epoch_loss_score(model, device, valid_loader, loss_func)\n",
    "loss_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, clip: np.ndarray,\n",
    "                 img_size=224, melspectrogram_parameters={}):\n",
    "        self.df = df\n",
    "        self.clip = clip\n",
    "        self.img_size = img_size\n",
    "        self.melspectrogram_parameters = melspectrogram_parameters\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        SR = 32000\n",
    "        sample = self.df.loc[idx, :]\n",
    "        site = sample.site\n",
    "        row_id = sample.row_id\n",
    "        \n",
    "        if site == \"site_3\":\n",
    "            y = self.clip.astype(np.float32)\n",
    "            len_y = len(y)\n",
    "            start = 0\n",
    "            end = SR * 5\n",
    "            images = []\n",
    "            while len_y > start:\n",
    "                y_batch = y[start:end].astype(np.float32)\n",
    "                if len(y_batch) != (SR * 5):\n",
    "                    break\n",
    "                start = end\n",
    "                end = end + SR * 5\n",
    "                \n",
    "                melspec = librosa.feature.melspectrogram(y_batch,\n",
    "                                                         sr=SR,\n",
    "                                                         **self.melspectrogram_parameters)\n",
    "                melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "                image = mono_to_color(melspec)\n",
    "                height, width, _ = image.shape\n",
    "                image = cv2.resize(image, (int(width * self.img_size / height), self.img_size))\n",
    "                image = np.moveaxis(image, 2, 0)\n",
    "                image = (image / 255.0).astype(np.float32)\n",
    "                images.append(image)\n",
    "            images = np.asarray(images)\n",
    "            return images, row_id, site\n",
    "        else:\n",
    "            end_seconds = int(sample.seconds)\n",
    "            start_seconds = int(end_seconds - 5)\n",
    "            \n",
    "            start_index = SR * start_seconds\n",
    "            end_index = SR * end_seconds\n",
    "            \n",
    "            y = self.clip[start_index:end_index].astype(np.float32)\n",
    "\n",
    "            melspec = librosa.feature.melspectrogram(y, sr=SR, **self.melspectrogram_parameters)\n",
    "            melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "\n",
    "            image = mono_to_color(melspec)\n",
    "            height, width, _ = image.shape\n",
    "            image = cv2.resize(image, (int(width * self.img_size / height), self.img_size))\n",
    "            image = np.moveaxis(image, 2, 0)\n",
    "            image = (image / 255.0).astype(np.float32)\n",
    "\n",
    "            return image, row_id, site\n",
    "        \n",
    "def prediction_for_clip(test_df: pd.DataFrame, \n",
    "                        clip: np.ndarray, \n",
    "                        model, \n",
    "                        mel_params: dict, \n",
    "                        threshold=0.5):\n",
    "\n",
    "    dataset = TestDataset(df=test_df, \n",
    "                          clip=clip,\n",
    "                          img_size=224,\n",
    "                          melspectrogram_parameters=mel_params)\n",
    "    loader = data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    prediction_dict = {}\n",
    "    for image, row_id, site in progress_bar(loader):\n",
    "        site = site[0]\n",
    "        row_id = row_id[0]\n",
    "        if site in {\"site_1\", \"site_2\"}:\n",
    "            image = image.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prediction = model(image)\n",
    "                proba = F.sigmoid(prediction).detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "            events = proba >= threshold\n",
    "            labels = np.argwhere(events).reshape(-1).tolist()\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # to avoid prediction on large batch\n",
    "            image = image.squeeze(0)\n",
    "            batch_size = 16\n",
    "            whole_size = image.size(0)\n",
    "            if whole_size % batch_size == 0:\n",
    "                n_iter = whole_size // batch_size\n",
    "            else:\n",
    "                n_iter = whole_size // batch_size + 1\n",
    "                \n",
    "            all_events = set()\n",
    "            for batch_i in range(n_iter):\n",
    "                batch = image[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "#                 if batch.ndim == 3:\n",
    "#                     batch = batch.unsqueeze(0)\n",
    "\n",
    "                batch = batch.to(device)\n",
    "                with torch.no_grad():\n",
    "                    prediction = model(batch)\n",
    "                    proba = F.sigmoid(prediction).detach().cpu().numpy()              \n",
    "                events = proba >= threshold\n",
    "                for i in range(len(events)):\n",
    "                    event = events[i, :]\n",
    "                    labels = np.argwhere(event).reshape(-1).tolist()\n",
    "                    for label in labels:\n",
    "                        all_events.add(label)\n",
    "                        \n",
    "            labels = list(all_events)\n",
    "        if len(labels) == 0:\n",
    "            prediction_dict[row_id] = \"nocall\"\n",
    "        else:\n",
    "            labels_str_list = list(map(lambda x: INV_BIRD_CODE[x], labels))\n",
    "            label_string = \" \".join(labels_str_list)\n",
    "            prediction_dict[row_id] = label_string\n",
    "    return prediction_dict\n",
    "\n",
    "def prediction(test_df: pd.DataFrame,\n",
    "               test_audio: Path,\n",
    "               mel_params: dict,\n",
    "               model,\n",
    "               threshold=0.5):\n",
    "    \n",
    "    unique_audio_id = test_df.audio_id.unique()\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    prediction_dfs = []\n",
    "    for audio_id in unique_audio_id:\n",
    "        with timer(f\"Loading {audio_id}\", logger):\n",
    "            clip, _ = librosa.load(f'{test_audio}/{audio_id}.mp3',\n",
    "                                   sr=TARGET_SR,\n",
    "                                   mono=True,\n",
    "                                   res_type=\"kaiser_fast\")\n",
    "        \n",
    "        test_df_for_audio_id = test_df.query(\n",
    "            f\"audio_id == '{audio_id}'\").reset_index(drop=True)\n",
    "        with timer(f\"Prediction on {audio_id}\", logger):\n",
    "            prediction_dict = prediction_for_clip(test_df_for_audio_id,\n",
    "                                                  clip=clip,\n",
    "                                                  model=model,\n",
    "                                                  mel_params=mel_params,\n",
    "                                                  threshold=threshold)\n",
    "        row_id = list(prediction_dict.keys())\n",
    "        birds = list(prediction_dict.values())\n",
    "        prediction_df = pd.DataFrame({\n",
    "            \"row_id\": row_id,\n",
    "            \"birds\": birds\n",
    "        })\n",
    "        prediction_dfs.append(prediction_df)\n",
    "    \n",
    "    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n",
    "    return prediction_df\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str, logger):\n",
    "    t0 = time.time()\n",
    "    msg = f\"[{name}] start\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "    yield\n",
    "\n",
    "    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "        \n",
    "def get_logger(out_file=None):\n",
    "    logger = logging.getLogger()\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logger.handlers = []\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        fh = logging.FileHandler(out_file)\n",
    "        fh.setFormatter(formatter)\n",
    "        fh.setLevel(logging.INFO)\n",
    "        logger.addHandler(fh)\n",
    "    logger.info(\"logger set up\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "set_seed(1213)\n",
    "\n",
    "dir_test_audio = '../data/external_dataset/birdcall-check/test_audio/'\n",
    "test = pd.read_csv('./../data/external_dataset/birdcall-check/test.csv')\n",
    "sub = pd.read_csv('./../data_ignore/official/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melspectrogram_parameters = {\n",
    "    \"n_mels\": 128,\n",
    "    \"fmin\": 20,\n",
    "    \"fmax\": 16000\n",
    "}\n",
    "\n",
    "THRESHOLD = 0.6\n",
    "TARGET_SR = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = prediction(test_df=test,\n",
    "                        test_audio=dir_test_audio,\n",
    "                        model=model,\n",
    "                        mel_params=melspectrogram_parameters,\n",
    "                        threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500\n",
    "submission.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
